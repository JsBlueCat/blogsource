---
title: 2DPCA
date: 2018-04-30 09:38:05
tags:
    - machine learning
categories: 
    - 特征提取
---

# Two-dimensional-PCA-a-new-approach-to-appearance-based-face-representation-and-recognition 读后感

## 概览
本文介绍了传统的 ICA 方法和 PCA 方法更好的 2DPCA 方法。2DPCA 是 2维的 PCA 识别。PCA 是 principal component analysis 缩写。 中文解释为特征分析
### 数据集
本文的数据集的人脸分析是存在于
ORL, AR, Yale face 这3种人脸数据库。

### PCA
主要是基于面部特征分类，重组，从而识别人类面部。
比较方法是基于协方差矩阵。
但是PCA有个缺点，他很难抓住特征点，除非特征点直接写在数据集中。
所以作者想了一种新的方法 elastic bunch
graph matching 一种基于特征值的算法

### ICA
是 independent component
analysis 的缩写，面部分割在拼接，这样做是不会影响正确率的在协方差的方面
### 比较
PCA 和 ICA  余弦方面ICA 比较的优秀，而如果使用了欧氏距离，2者相似

时间复杂度方面  ICA > K PCA > PCA 

## 2DPCA
作者现在 发现PCA 很难把高维的图片抽象成一个协方差矩阵 因为 他的训练集的相关和图片大小

为了解决这种问题，作者引入了 SVD 奇异值分解

*ps：不懂的人，去恶补线代.看不懂，别学了，退学！（zzy名言）*

现在 2DPCA 可以直接抽象出面部特征，因为2DPCA 直接基于2维数组，而不是一维的向量。2维不需要直接转换成1维的

### 算法
*终于开始研究算法，前面都是废话。好了开始上算法，老铁抓稳了~*

我们现在直接基于 A 是给定的图像矩阵 n*m维度的
$$
Y = AX
$$
其中X为 m维的一个列向量，我们把A 基于X方向的投影抽象成一个矩阵Y，Y就是抽象出来的特征的投影向量。我们的目标是来评估一个协方差矩阵，最后根据协方差来划定阈值，判断图片是否为对应的图片。

那么我们的目标就是找到这个X方向的投影向量，通俗来说就是列变换向量。

这里引入了
$$
J(X) = tr(S_x)
$$
\\(S_x\\) 是指协方差矩阵。
*那么什么是协方差矩阵呢？小伙伴又不明白了！*
#### 协防差矩阵
这里引入了 wikipedia 上的协方差定义
![](/images/cov1.svg) 
*泪崩，不由得惊叹考研害死人啊，惯性思维*
印象中 
$$
COV(X,Y)=E(XY)-E(X)\*E(Y)
$$
*是不是很坑？*

而协方差举证 就是 每一个
$$
COV(X_i,X_j)
$$
就是类似于
![](/images/covm.svg)

*为什么有个=号呢，请原谅我懒不高兴自己写*

好了，那个这个协方差矩阵就好办了 
$$
\Sigma = E[(X-E[X])*(X-E[X])^{T}] 
$$
附上特性
![](/images/covmc.png)
### 求解\\(S_x\\)

$$
S_x = E[(Y-E[Y])(Y-E[Y])^T]=E[[AX-E(AX)][AX-E(AX)]^T]=E[((A-EA)X)([A-EA]X)^T]
$$
我们来求迹
$$
tr(S_X)=X^T[E(A-EA)^T(A-EA)]X
$$
其中 X为列向量， 一个列向量的迹是
\\(X^TX\\)  
线代基础
协方差的特性是他是个列向量和行向量的乘积矩阵那么做就行了

现在我们令
$$
G_t=E[(A-EA)^T(A-EA)]
$$
其中E就是期望了，就是A的均值，我们以求和的方式重新定义G
$$
G_t=\dfrac {1}{M}\sum ^{M}_{j=1}(A_j-\overline {A})^T(A_j-\overline {A})
$$

\\(\overline {A}\\) 代表着M个图片的均值

好了重写下 J(X)
$$
J(X) = X^TG_tX
$$
X是标准列向量，J(X)称为广义扩散准则
一个J(X) 是不够的 所以需要一组的 Xi 然后去优化他的，Xi 和 Xj 是对应正交的

\\(G_t\\) 对应前d个特征向量值

### 提取特征
$$
[Y_1,Y_2,Y_3,...]^T = A[X_1,X_2,X_3,..]^T
$$

这样总的矩阵Y 就是一个 2维度的，他的特征就是一个一维的向量，而PCA的特征只是一个点

### 基于欧式距离的协方差矩阵判断

$$
B_i = [Y^{(i)}_1,Y^{(i)}_2,...]
$$

$$
B_j = [Y^{(j)}_1,Y^{(j)}_2,...]
$$

$$
d(B_i,B_j)=\sum^d_{k=1}
<!-- \left\|Y^{i}_{k}-Y^{j}_{k}\right\|_{2} -->
$$

如果 
$$
d(B,B_l)=mind(B,B_j)
$$
\\(B_l\\)是给定的一个正确的图片，如果B和\\(B_j\\)的欧式距离和最优的欧式距离相等，就认为B是正确的

## 基于2DPCA 的图像识别
把\\(G_t\\) 的最大的前d个特征值对饮的特征项向量 记为 X(1-k) 记为U  然后对应的 Y(1-k) 记为 V

$$
V=[Y_1,Y_2,Y_3,...] 
U = [X_1,X_2,X_3,..]
$$

$$
    V = AU
$$

X(1-k) 是正交的

所以 可以把 A 重组出来的

$$
\begin{aligned}\sim \\\\ D\end{aligned}  = VU^T = \sum^d_{k=1} Y_k X^T_k 
$$

其中如果 d = n 就能还原 A 图像，不丢失任何信息，而如果 d < n 那么就能得到 和 A 近似的矩阵

## 实验数据集
基于 ORL 数据库的 的图像比较.

实验证明，最大的几个特征值，能够使还原的图像接近于原图，所以我们可以用 前k个特征向量去替换A


## 对比
正确率
![](/images/2dpcaa.png)

时间
![](/images/2dpcat.png)

和其他算法的对比
![](/images/2dpcaica.png)

## 总结
2DPCA 的优点主要抽象了个特征维度的矩阵，使用了特征向量正交的特性，使计算的复杂性变的很低，对于 矩阵的n次方。 n 如果很大会接近一个 特定的矩阵 是和他最大的特征向量相似。

其实这个算法就是基于了这个特性，把特征值抽象出来，因为是可逆线性变换所以可以无损还原原图，如果矩阵非满秩，把最大的特征值抽象出来，是可以最大程度接近原图，这样的 一维的计算复杂度 就可以大大的降低

为什么正确率优化了呢，因为使用了全局的贪心算法，使得 抽象出来的n个特征值，基于欧式距离的值最短

